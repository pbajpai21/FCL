# LoRA Quick Reference

## ğŸ¯ One-Sentence Summary

**LoRA adds tiny trainable adapter matrices to a frozen base model, allowing task-specific adaptation with minimal parameters.**

---

## ğŸ“ The Math in Simple Terms

### Standard Training:
```
h = W Ã— x
â†‘  â†‘   â†‘
â”‚  â”‚   â””â”€â”€â”€ Input
â”‚  â””â”€â”€â”€â”€â”€â”€â”€ Weight matrix (trainable, large)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Output
```

### With LoRA:
```
h = W Ã— x + (B Ã— A) Ã— x
  â†‘   â†‘     â†‘   â†‘    â†‘
  â”‚   â”‚     â”‚   â”‚    â””â”€â”€â”€ Input
  â”‚   â”‚     â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€ Small adapter matrix A
  â”‚   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Small adapter matrix B
  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Frozen weight matrix
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Output
```

**Key**: `B Ã— A` is much smaller than `W`!

---

## ğŸ”¢ Concrete Example

### Scenario: Linear Layer with 1000 inputs, 1000 outputs

**Standard Approach:**
- Weight matrix `W`: 1000 Ã— 1000 = **1,000,000 parameters**
- All parameters trainable

**LoRA Approach:**
- Base weight `W`: 1000 Ã— 1000 = **1,000,000 parameters** (frozen)
- Adapter `A`: 1000 Ã— 10 = **10,000 parameters** (trainable)
- Adapter `B`: 10 Ã— 1000 = **10,000 parameters** (trainable)
- **Total trainable**: 20,000 parameters (50x reduction!)

**Memory Savings:**
- Standard: 1M params Ã— 4 bytes = 4MB per task
- LoRA: 20K params Ã— 4 bytes = 80KB per task
- **50x less memory per task!**

---

## ğŸ¨ Visual Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BASE MODEL (Frozen)                  â”‚
â”‚                                                          â”‚
â”‚   Input â”€â”€â”€â†’ [Linear] â”€â”€â”€â†’ [Linear] â”€â”€â”€â†’ [Linear] â”€â”€â”€â†’ Output â”‚
â”‚              (Wâ‚)         (Wâ‚‚)         (Wâ‚ƒ)            â”‚
â”‚              â„ï¸ frozen    â„ï¸ frozen    â„ï¸ frozen       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           +
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LoRA ADAPTERS (Trainable)                    â”‚
â”‚                                                          â”‚
â”‚   Input â”€â”€â”€â†’ [Bâ‚Aâ‚] â”€â”€â”€â†’ [Bâ‚‚Aâ‚‚] â”€â”€â”€â†’ [Bâ‚ƒAâ‚ƒ] â”€â”€â”€â†’ Add    â”‚
â”‚              (tiny)      (tiny)      (tiny)            â”‚
â”‚              ğŸ”¥ train    ğŸ”¥ train    ğŸ”¥ train           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Continual Learning Workflow

### Task 1:
```
1. Start with base model W (pretrained or trained on Task 1)
2. Freeze W
3. Add LoRA adapters: Aâ‚, Bâ‚
4. Train only Aâ‚, Bâ‚ on Task 1
5. Save adapters: LoRAâ‚ = {Aâ‚, Bâ‚}
```

### Task 2:
```
1. Use same base model W (still frozen)
2. Add NEW LoRA adapters: Aâ‚‚, Bâ‚‚
3. Train only Aâ‚‚, Bâ‚‚ on Task 2
4. Save adapters: LoRAâ‚‚ = {Aâ‚‚, Bâ‚‚}
5. LoRAâ‚ still exists and works!
```

### Inference:
```
Task 1: Forward pass with W + Bâ‚Aâ‚
Task 2: Forward pass with W + Bâ‚‚Aâ‚‚
Task 3: Forward pass with W + Bâ‚ƒAâ‚ƒ
...
```

---

## ğŸ“Š Parameter Comparison Table

| Method | Params (1000Ã—1000 layer) | Trainable | Memory/Task |
|--------|-------------------------|-----------|-------------|
| **Full Fine-tune** | 1M | 1M | 4MB |
| **Freeze All** | 1M | 0 | 0MB (but can't learn) |
| **LoRA (r=4)** | 1M (frozen) + 8K | 8K | 32KB |
| **LoRA (r=8)** | 1M (frozen) + 16K | 16K | 64KB |
| **LoRA (r=16)** | 1M (frozen) + 32K | 32K | 128KB |

**Key Insight**: Even with r=16, LoRA uses **31x fewer trainable parameters!**

---

## ğŸ¯ Key Hyperparameters

### 1. **Rank (r)**
- **Low (1-4)**: Maximum efficiency, may limit performance
- **Medium (8-16)**: Good balance (recommended starting point)
- **High (32+)**: Better performance, less efficient

### 2. **Alpha (Î±)**
- Scaling factor: `output = Wx + (Î±/r) Ã— BAx`
- Typically: `Î± = r` or `Î± = 2r`
- Controls adapter influence strength

### 3. **Which Layers**
- **All layers**: Maximum adaptation
- **Attention only**: Efficient (for transformers)
- **Last layers only**: Task-specific adaptation

---

## ğŸ’¡ Why It Works

### 1. **Low-Rank Hypothesis**
Research shows: Weight updates in fine-tuning often have **low intrinsic dimensionality**.

Example:
- You have 1M parameters
- But the "direction" of updates lies in ~10-dimensional space
- So you only need to learn 10 dimensions worth of changes!

### 2. **Efficient Representation**
Instead of:
```
Î”W = [1M values]
```

LoRA uses:
```
Î”W â‰ˆ BA = [small A] Ã— [small B] = [~20K values total]
```

### 3. **Task Isolation**
- Each task's adapter is independent
- No interference between tasks
- Can remove/add tasks easily

---

## âœ… Advantages

1. **Efficient**: Train 10-100x fewer parameters
2. **Fast**: Small adapters train quickly
3. **Isolated**: Tasks don't interfere
4. **Scalable**: Can handle many tasks
5. **Modular**: Easy to add/remove tasks

---

## âš ï¸ Limitations

1. **Rank Selection**: Need to choose appropriate rank
2. **Base Model**: Quality depends on base model
3. **Low-Rank Assumption**: May not hold for all tasks
4. **Hyperparameter Tuning**: Need to tune r and Î±

---

## ğŸ”— Comparison with Other Methods

| Method | How It Works | When Tasks Differ |
|--------|--------------|-------------------|
| **EWC** | Regularize weight changes | Still updates all weights |
| **ER** | Replay old data | Uses memory buffer |
| **PNN** | Add full columns | Works but very expensive |
| **Shared-Private** | Freeze encoder, add heads | Assumes general features |
| **LoRA** | Add tiny adapters | Works efficiently |

---

## ğŸ“ Key Takeaway

**LoRA = "Surgical Updates"**

Instead of changing the entire model (expensive, risky), LoRA makes tiny targeted changes via small adapter matrices. It's like:
- **Full fine-tuning**: Renovating entire house
- **LoRA**: Just updating the door handles (much cheaper, faster, less disruptive!)

---

## ğŸ“ Code Pseudocode

```python
# Forward pass with LoRA
def forward_with_lora(x, W, A, B, alpha=8, rank=8):
    # Base model output (frozen)
    base_output = W @ x
    
    # LoRA adapter output (trainable)
    adapter_output = (alpha / rank) * (B @ (A @ x))
    
    # Combined
    return base_output + adapter_output

# Training: Only update A and B, not W
optimizer = Adam([A, B])  # W not in optimizer!
```

---

## ğŸš€ Next Steps

1. Understand the low-rank decomposition concept
2. See how adapters are initialized (A random, B zero)
3. Learn about rank selection strategies
4. Implement LoRA for continual learning!

