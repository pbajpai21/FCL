# LoRA (Low-Rank Adaptation) for Continual Learning

## ðŸ“š Overview

**LoRA (Low-Rank Adaptation)** is a parameter-efficient fine-tuning technique that was originally developed for adapting large language models (like GPT) to new tasks. However, it's also highly effective for continual learning!

**Key Idea**: Instead of updating all weights, add small "adapter" matrices that can be efficiently learned and swapped.

---

## ðŸŽ¯ Core Concept

### The Problem It Solves

When you have a large model with millions of parameters:
- **Full fine-tuning**: Update all weights â†’ Expensive, can cause catastrophic forgetting
- **Freeze everything**: No learning â†’ Can't adapt to new tasks
- **LoRA**: Add tiny learnable adapters â†’ Best of both worlds!

### The Mathematical Foundation

LoRA is based on a fundamental insight: **weight updates in neural networks often have low intrinsic rank**.

#### What Does "Low Rank" Mean?

In linear algebra, a matrix has "low rank" if it can be decomposed into smaller matrices:

```
Large matrix W (d Ã— d) â‰ˆ Small matrix A (d Ã— r) Ã— Small matrix B (r Ã— d)
                        where r << d
```

**Example**:
- Original weight matrix: `W` = 1000 Ã— 1000 = **1M parameters**
- Low-rank decomposition: `A` = 1000 Ã— 10 + `B` = 10 Ã— 1000 = **20K parameters**
- Reduction: 50x fewer parameters!

---

## ðŸ”¬ How LoRA Works

### Standard Training (Without LoRA)

```
Input â†’ W (full weight matrix) â†’ Output
       â†‘
    All weights updated during training
```

### With LoRA

```
Input â†’ W (frozen) â†’ + â†’ Output
       â†‘            â†‘
    Frozen      LoRA adapter (A Ã— B)
                (small, trainable)
```

**The Forward Pass**:
```
h = Wx + Î”Wx
  = Wx + (BA)x
  = Wx + B(Ax)
```

Where:
- `W`: Original frozen weight matrix
- `Î”W = BA`: Low-rank adaptation (product of two small matrices)
- `A`: Down-projection matrix (d Ã— r), trainable
- `B`: Up-projection matrix (r Ã— d), trainable
- `r`: Rank (typically 1-16, very small!)

---

## ðŸŽ¨ Visual Understanding

### Standard Fine-Tuning:
```
Task 1: [Full Model] â”€â”€â†’ Output 1
        All 10M params trainable

Task 2: [Full Model] â”€â”€â†’ Output 2  
        All 10M params trainable (overwrites Task 1!)
```
âŒ Problem: Task 1 knowledge overwritten

### LoRA Approach:
```
Task 1: [Base Model (frozen)] + [LoRAâ‚ (0.1M params)] â”€â”€â†’ Output 1
        (10M params)         (tiny adapter)

Task 2: [Base Model (frozen)] + [LoRAâ‚‚ (0.1M params)] â”€â”€â†’ Output 2
        (10M params)         (different adapter, Task 1 adapter preserved)
```
âœ… Solution: Base model frozen, only small adapters learned per task!

---

## ðŸ” Detailed Mechanism

### Step-by-Step:

1. **Initialize Base Model**
   - Train or load a pretrained model
   - Freeze all original weights `W`

2. **Add LoRA Adapters**
   - For each linear layer, add two small matrices:
     - `A` (d Ã— r): Random initialization
     - `B` (r Ã— d): Zero initialization (so `BA = 0` initially)
   - Only `A` and `B` are trainable

3. **Forward Pass with LoRA**
   ```
   Original: h = Wx
   With LoRA: h = Wx + BAx
                â†‘    â†‘
            frozen  trainable
   ```

4. **Training**
   - Only update `A` and `B` matrices
   - Original weights `W` remain frozen

5. **Task Switching**
   - Task 1: Use `W + Bâ‚Aâ‚`
   - Task 2: Use `W + Bâ‚‚Aâ‚‚`
   - Can store/load adapters per task!

---

## ðŸ’¡ Why LoRA Works for Continual Learning

### 1. **Parameter Efficiency**
- Only train ~0.1-1% of parameters
- Can store many task adapters
- Fast training

### 2. **Isolation**
- Each task gets its own adapter
- Tasks don't interfere (base model frozen)
- Can switch adapters at inference time

### 3. **Low-Rank Hypothesis**
- Weight updates often lie in low-dimensional subspaces
- Small adapters capture task-specific changes efficiently

### 4. **Flexibility**
- Can apply to any layer
- Can apply to all layers or just some
- Tunable rank `r` for efficiency/performance tradeoff

---

## ðŸ“Š Comparison with Other Methods

| Method | Params per Task | Memory | Forgetting | Flexibility |
|--------|----------------|--------|------------|-------------|
| **Full Fine-tuning** | All (10M) | Low | High | High |
| **Freeze Everything** | 0 | Low | None | None |
| **Shared-Private** | Small heads (~1K) | Low | Low | Medium |
| **PNN** | Full column (~10M) | High | None | High |
| **LoRA** | Tiny adapters (~100K) | Low | Low | High |

### Key Advantages of LoRA:

âœ… **Parameter Efficient**: Only train tiny adapters
âœ… **No Forgetting**: Base model never changes
âœ… **Scalable**: Store many adapters (one per task)
âœ… **Fast**: Small adapters train quickly
âœ… **Flexible**: Can apply selectively to layers

---

## ðŸŽ¯ When to Use LoRA

### âœ… Best For:
1. **Large Models**: When full fine-tuning is expensive
2. **Many Tasks**: When you'll learn many tasks sequentially
3. **Parameter Constraints**: Limited memory/storage
4. **Quick Adaptation**: Need fast task switching
5. **Transfer Learning**: Strong base model available

### âŒ Less Suitable For:
1. **Very Small Models**: Overhead not worth it
2. **Dramatically Different Tasks**: Base model may be insufficient
3. **Tasks Requiring Major Changes**: Low-rank may be too restrictive

---

## ðŸ”¬ Mathematical Deep Dive

### Low-Rank Decomposition Theorem

**Given**: A weight update matrix `Î”W` of size (d Ã— d)

**Low-Rank Approximation**:
```
Î”W â‰ˆ BA

Where:
- B: (d Ã— r) matrix
- A: (r Ã— d) matrix  
- r << d (rank, typically 1-16)
```

**Parameter Count**:
- Original `Î”W`: dÂ² parameters
- LoRA `BA`: 2dr parameters
- **Reduction factor**: dÂ² / 2dr = d / 2r

**Example**: d=1000, r=10
- Original: 1,000,000 parameters
- LoRA: 20,000 parameters
- **50x reduction!**

### Why Zero Initialize B?

```python
# Standard LoRA initialization
A = random_normal(mean=0, std=1/r)  # Random
B = zeros()                          # Zeros!
```

**Reason**: So that `BA = 0` initially:
- Model starts identical to base model
- No disruption from random initialization
- Clean task adaptation

### Gradient Flow

During backpropagation:
```
Loss â†’ Output â†’ (Wx + BAx) â†’ Input
                â†‘    â†‘
              frozen trainable
```

Only `A` and `B` receive gradients, `W` does not!

---

## ðŸ—ï¸ Architecture Design Choices

### 1. **Which Layers to Apply LoRA?**

**Option A: All Linear Layers**
- Most comprehensive
- Higher parameter count
- Best performance potentially

**Option B: Only Key Layers**
- Attention layers (in transformers)
- Final classification layers
- More efficient

**Option C: Task-Specific Choice**
- Analyze which layers matter most
- Apply LoRA selectively

### 2. **Rank Selection (r)**

| Rank (r) | Params | Performance | Speed |
|----------|--------|-------------|-------|
| 1 | Minimal | May be limiting | Fastest |
| 4-8 | Low | Good balance | Fast |
| 16 | Moderate | High | Medium |
| 32+ | High | Very high | Slower |

**Rule of thumb**: Start with r=8, adjust based on task complexity

### 3. **Alpha Parameter (Î±)**

LoRA often includes a scaling factor:
```
h = Wx + (Î±/r) Â· BAx
```

Where `Î±` is a hyperparameter (typically = r or 2r)
- Controls adapter influence
- Helps with numerical stability

---

## ðŸ”„ LoRA in Continual Learning Workflow

### Training Phase:

```
Task 1:
1. Base model W (frozen)
2. Create LoRAâ‚: Aâ‚, Bâ‚
3. Train: Only update Aâ‚, Bâ‚
4. Save LoRAâ‚

Task 2:
1. Base model W (same, frozen)
2. Create LoRAâ‚‚: Aâ‚‚, Bâ‚‚ (new adapters)
3. Train: Only update Aâ‚‚, Bâ‚‚
4. Save LoRAâ‚‚
5. LoRAâ‚ preserved!

Task 3:
... (repeat)
```

### Inference Phase:

```
For Task 1: Use W + Bâ‚Aâ‚
For Task 2: Use W + Bâ‚‚Aâ‚‚
For Task 3: Use W + Bâ‚ƒAâ‚ƒ
...
```

Can switch adapters dynamically!

---

## ðŸ“ˆ Advantages for Continual Learning

### 1. **Zero Forgetting Guarantee**
- Base model `W` never changes
- Old adapters preserved
- Perfect isolation

### 2. **Efficiency**
- Small adapters = fast training
- Minimal memory overhead
- Scalable to many tasks

### 3. **Modularity**
- Adapters are independent modules
- Easy to add/remove tasks
- Can compose adapters (multi-task)

### 4. **Transfer Learning Friendly**
- Start with strong pretrained base
- Only adapt what's needed
- Leverages pretrained knowledge

---

## âš ï¸ Limitations & Considerations

### 1. **Low-Rank Assumption**
- Assumes updates are low-rank
- May not hold for all tasks
- Some tasks need higher rank

### 2. **Base Model Dependency**
- Quality depends on base model
- If base is poor, adapters can't fix it
- May need better base for diverse tasks

### 3. **Rank Selection**
- Need to tune rank `r`
- Too small: insufficient capacity
- Too large: defeats efficiency purpose

### 4. **Layer Selection**
- Need to decide which layers to adapt
- Not always obvious
- Some experimentation needed

---

## ðŸŽ“ Key Insights

1. **Low-Rank Hypothesis**: Most weight updates lie in low-dimensional subspaces
2. **Parameter Efficiency**: Train tiny adapters instead of full model
3. **Task Isolation**: Each task gets independent adapter
4. **Scalability**: Can handle many tasks efficiently
5. **Practical**: Works well in practice, especially for large models

---

## ðŸ”— Relationship to Other Methods

### LoRA vs. Shared-Private:
- **Shared-Private**: Adds heads, shares encoder
- **LoRA**: Adds adapters to existing layers
- **Both**: Parameter-efficient, representation-based

### LoRA vs. EWC:
- **EWC**: Regularizes weight changes
- **LoRA**: Avoids weight changes entirely (adapter approach)
- **LoRA**: More isolation, less interference

### LoRA vs. PackNet:
- **PackNet**: Masks weights in same architecture
- **LoRA**: Adds adapters, base weights unchanged
- **Both**: Fixed architecture, parameter-efficient

---

## ðŸ“š Further Reading

1. **Original LoRA Paper**: "LoRA: Low-Rank Adaptation of Large Language Models" (Hu et al., 2021)
2. **QLoRA**: Quantized LoRA for even more efficiency
3. **AdaLoRA**: Adaptive rank selection
4. **DoRA**: Decomposed LoRA for better performance

---

## ðŸŽ¯ Summary

**LoRA** is a powerful technique that:
- Adds small trainable adapters to frozen base model
- Reduces parameters by 10-100x
- Provides perfect task isolation
- Scales efficiently to many tasks
- Works especially well with large pretrained models

**Key Formula**: `output = base_model(x) + adapter(x) = Wx + BAx`

**Philosophy**: "Don't change the base model, just add tiny task-specific tweaks!"

